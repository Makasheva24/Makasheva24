

| **üü¢ Basic (–ù–æ–≤–∏—á–æ–∫)** | ‚úÖ –û—Å–Ω–æ–≤—ã ML (—á—Ç–æ —Ç–∞–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏)  <br>‚úÖ –õ–∏–Ω–µ–π–Ω–∞—è –∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è  <br>‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (scikit-learn, TensorFlow) |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |

## üíº –ü—Ä–æ–µ–∫—Ç: **"AI-–†–µ–∫—Ä—É—Ç–µ—Ä: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–π–º–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"**

### üîç –°—É—Ç—å:

–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç: **–Ω–∞–Ω–∏–º–∞—é—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏–ª–∏ –Ω–µ—Ç** –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∑—é–º–µ (–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –æ–ø—ã—Ç, –Ω–∞–≤—ã–∫–∏, soft skills –∏ —Ç.–¥.)

---

### üß† –¢—ã –æ—Å–≤–æ–∏—à—å:

| –¢–µ–º–∞ | –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è |
| --- | --- |
| ‚úÖ **–û—Å–Ω–æ–≤—ã ML** | –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ pipeline, –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –º–µ—Ç—Ä–∏–∫–∏ |
| ‚úÖ **–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** | –ü—Ä–æ–≥–Ω–æ–∑ –∑–∞—Ä–ø–ª–∞—Ç—ã –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ |
| ‚úÖ **–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–π–º–∞ (–¥–∞/–Ω–µ—Ç) |
| ‚úÖ **Scikit-learn** | –í–µ—Å—å pipeline, –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥, –æ–±—É—á–µ–Ω–∏–µ, –º–µ—Ç—Ä–∏–∫–∏ |
| ‚úÖ **TensorFlow** | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–π –∂–µ –º–æ–¥–µ–ª–∏ ‚Äî –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ GPU |
| ‚úÖ **–†–∞–±–æ—Ç–∞ —Å GPU** | –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ TensorFlow —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º |
| ‚úÖ **Pandas, Matplotlib** | –ß–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è |

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

# 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
np.random.seed(42)
n_samples = 1000

data = {
    "age": np.random.randint(20, 50, size=n_samples),
    "experience_years": np.random.randint(0, 15, size=n_samples),
    "education_level": np.random.choice(["High School", "Bachelor", "Master", "PhD"], size=n_samples),
    "knows_python": np.random.choice([0, 1], size=n_samples),
    "has_soft_skills": np.random.choice([0, 1], size=n_samples),
    "ml_course_completed": np.random.choice([0, 1], size=n_samples),
    "industry": np.random.choice(["Finance", "Tech", "Healthcare", "Education"], size=n_samples),
    "region": np.random.choice(["North", "South", "West", "East"], size=n_samples),
    "salary_expectation": np.random.randint(30000, 120000, size=n_samples),
}

prob_hired = (
    0.2 * data["experience_years"] +
    0.3 * data["knows_python"] +
    0.2 * data["ml_course_completed"] +
    0.2 * data["has_soft_skills"] +
    np.random.normal(0, 0.1, n_samples)
)
data["is_hired"] = (prob_hired > np.percentile(prob_hired, 60)).astype(int)

df = pd.DataFrame(data)

# 2. OneHot Encoding
df = pd.get_dummies(df, columns=["education_level", "industry", "region"])

# 3. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
numeric_cols = ["age", "experience_years", "knows_python", "has_soft_skills", "ml_course_completed", "salary_expectation"]
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# 4. –î–µ–ª–∏–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫—É
X = df.drop("is_hired", axis=1)
y = df["is_hired"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LogisticRegression()
model.fit(X_train, y_train)

# 6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
conf_matrix = confusion_matrix(y_test, predictions)

# 7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
metrics = {
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall
}
plt.figure(figsize=(6, 4))
plt.bar(metrics.keys(), metrics.values())
plt.title("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏")
plt.ylim(0, 1)
plt.ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ")
plt.grid(True)
plt.tight_layout()
plt.show()

(conf_matrix, accuracy, precision, recall)
```

–ü—Ä–∏–≤–µ—Ç! –†–∞–¥ –ø–æ–º–æ—á—å üòä –î–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä—ë–º—Å—è —Å —ç—Ç–∏–º–∏ —ç—Ç–∞–ø–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:

---

### üé≤ **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**

–≠—Ç–æ —ç—Ç–∞–ø, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º **—Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∏–ª–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ**, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å.

- –í —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–æ–≤, –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, API –∏–ª–∏ —Å–µ–Ω—Å–æ—Ä–æ–≤.
- –í —É—á–µ–±–Ω—ã—Ö —Ü–µ–ª—è—Ö —á–∞—Å—Ç–æ **–≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é `sklearn.datasets.make_classification` –∏–ª–∏ `make_regression`).
- –¶–µ–ª—å ‚Äî –ø–æ–ª—É—á–∏—Ç—å **–Ω–∞–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤**, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (features) –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (target), –Ω–∞–ø—Ä–∏–º–µ—Ä:
    
    ```
    X = [[1.2, 3.4], [2.1, 1.1], ...]   # –ø—Ä–∏–∑–Ω–∞–∫–∏
    y = [0, 1, 0, 1, ...]               # –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è
    
    ```
    

---

### üß† **–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Feature Engineering / Preprocessing)**

–ü–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ **–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ**:

- **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** (–Ω–∞–ø—Ä–∏–º–µ—Ä, StandardScaler, MinMaxScaler)
- **–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤** (`fillna()` –∏–ª–∏ `SimpleImputer`)
- **–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** (OneHotEncoding, LabelEncoding)
- **–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**

–¶–µ–ª—å ‚Äî –ø—Ä–∏–≤–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω—è—Ç–µ–Ω –º–æ–¥–µ–ª–∏ –∏ —É–ª—É—á—à–∞–µ—Ç –µ—ë —Ä–∞–±–æ—Ç—É.

---

### ‚öôÔ∏è **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏**

–ó–¥–µ—Å—å –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç **—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö**:

- –î–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ **–æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏** (`train_test_split`)
- –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, `LogisticRegression`, `RandomForestClassifier`)
- –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (`model.fit(X_train, y_train)`)
- –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (`model.predict(X_test)`)

---

### üìä **–ú–µ—Ç—Ä–∏–∫–∏**

–ú–µ—Ç—Ä–∏–∫–∏ –Ω—É–∂–Ω—ã –¥–ª—è **–æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏**:

- –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: `accuracy`, `precision`, `recall`, `f1-score`, `ROC-AUC`
- –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: `MAE`, `MSE`, `RMSE`, `R¬≤`
- –ü—Ä–∏–º–µ—Ä:
    
    ```python
    from sklearn.metrics import accuracy_score
    accuracy_score(y_test, y_pred)
    
    ```
    

---

–•–æ—á–µ—à—å –ø—Ä–∏–º–µ—Ä –Ω–∞ Python, –≥–¥–µ –≤—Å—ë —ç—Ç–æ –ø–æ–∫–∞–∑–∞–Ω–æ –ø–æ—à–∞–≥–æ–≤–æ?



| **üîµ Medium (–°—Ä–µ–¥–Ω–∏–π)** | ‚úÖ –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π, —Å–ª—É—á–∞–π–Ω—ã–µ –ª–µ—Å–∞ (Decision Trees, Random Forest) <br>‚úÖ KNN, SVM <br>‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (K-Means, DBSCAN) |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------- |
### ‚úÖ **–ü—Ä–æ–µ–∫—Ç 1: AI-–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Å–æ–≤–µ—Ç–Ω–∏–∫: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è, –ø—Ä–æ–≥–Ω–æ–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤**

**–ß—Ç–æ –≤–Ω—É—Ç—Ä–∏:**

- **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (K-Means, DBSCAN)**: –ù–∞—Ö–æ–¥–∏—à—å –∫–ª–∞—Å—Ç–µ—Ä—ã —Å—Ä–µ–¥–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –ø–æ–≤–µ–¥–µ–Ω–∏—é (–¥–æ—Ö–æ–¥—ã, —Ä–∞—Å—Ö–æ–¥—ã, —Ü–µ–ª–∏, —Ä–∏—Å–∫-–ø—Ä–æ—Ñ–∏–ª—å).
- **KNN/SVM**: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—à—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –∫ –∫–ª–∞—Å—Ç–µ—Ä—É (–ø–æ—Ä—Ç—Ñ–µ–ª—å –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞).
- **Random Forest/Decision Tree**: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—à—å, –∫–∞–∫–æ–π –ø—Ä–æ–¥—É–∫—Ç –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫–ª–∏–µ–Ω—Ç—É (–æ–±–ª–∏–≥–∞—Ü–∏–∏, –∞–∫—Ü–∏–∏, –∫—Ä–∏–ø—Ç–∞ –∏ —Ç.–¥.).
- **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D/3D**: –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—à—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.

**–ü–æ—á–µ–º—É —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:**

- –û—á–µ–Ω—å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–æ –∫ —Ä–µ–∞–ª—å–Ω–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ (—Ñ–∏–Ω–∞–Ω—Å—ã + ML), –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã, –¥–æ–±–∞–≤–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ñ–µ–π–∫–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.

```python

# ‚úÖ –ü—Ä–æ–µ–∫—Ç: AI-–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Å–æ–≤–µ—Ç–Ω–∏–∫: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è, –ø—Ä–æ–≥–Ω–æ–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤

# üîß –ò–º–ø–æ—Ä—Ç—ã
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import networkx as nx

# üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
np.random.seed(42)
n_clients = 100_000

ages = np.random.randint(18, 65, size=n_clients)
incomes = np.random.randint(150_000, 1_500_000, size=n_clients)
expenses = np.random.randint(50_000, 800_000, size=n_clients)
risk_profiles = np.random.choice(["–ù–∏–∑–∫–∏–π", "–°—Ä–µ–¥–Ω–∏–π", "–í—ã—Å–æ–∫–∏–π"], size=n_clients, p=[0.4, 0.4, 0.2])
investment_goals = np.random.choice(["–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å", "–†–æ—Å—Ç –∫–∞–ø–∏—Ç–∞–ª–∞", "–ü–∞—Å—Å–∏–≤–Ω—ã–π –¥–æ—Ö–æ–¥"], size=n_clients)

df = pd.DataFrame({
    "age": ages,
    "income": incomes,
    "expenses": expenses,
    "risk_profile": risk_profiles,
    "investment_goal": investment_goals
})

# üí° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
df['recommendation'] = df['risk_profile'].map({
    "–í—ã—Å–æ–∫–∏–π": "–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞",
    "–°—Ä–µ–¥–Ω–∏–π": "–ê–∫—Ü–∏–∏",
    "–ù–∏–∑–∫–∏–π": "–û–±–ª–∏–≥–∞—Ü–∏–∏"
})

# üóÉÔ∏è –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
encoded_df = df.copy()
le_risk = LabelEncoder()
le_goal = LabelEncoder()
encoded_df['risk_profile_enc'] = le_risk.fit_transform(encoded_df['risk_profile'])
encoded_df['investment_goal_enc'] = le_goal.fit_transform(encoded_df['investment_goal'])

# ‚ôªÔ∏è –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
features = encoded_df[['age', 'income', 'expenses', 'risk_profile_enc', 'investment_goal_enc']]
kmeans = KMeans(n_clusters=3, random_state=42)
encoded_df['cluster'] = kmeans.fit_predict(features)

# üß¨ –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
cluster_summary = encoded_df.groupby('cluster').agg({
    'age': ['mean', 'min', 'max'],
    'income': ['mean', 'min', 'max'],
    'expenses': ['mean', 'min', 'max'],
    'risk_profile': lambda x: x.value_counts().to_dict(),
    'investment_goal': lambda x: x.value_counts().to_dict()
})
cluster_summary.columns = ['_'.join(col).strip() for col in cluster_summary.columns.values]
cluster_summary.reset_index(inplace=True)
print("\\nüìä –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.expand_frame_repr', False)

print(cluster_summary)

# ü™™ –°–æ–∑–¥–∞–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø
bins = [18, 25, 35, 45, 55, 65]
labels = ["18-25", "26-35", "36-45", "46-55", "56-65"]
encoded_df['age_group'] = pd.cut(encoded_df['age'], bins=bins, labels=labels, right=False)

# üí∞ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Å—Ç–∞—Ç–æ–∫
encoded_df['portfolio_balance'] = encoded_df['income'] - encoded_df['expenses']

# üß© –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
age_group_summary = encoded_df.groupby('age_group').agg(
    clients=('age', 'count'),
    avg_income=('income', 'mean'),
    avg_expenses=('expenses', 'mean'),
    avg_balance=('portfolio_balance', 'mean'),
    total_balance=('portfolio_balance', 'sum')
).reset_index()
print("\\nüìä –§–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–º:")
print(age_group_summary)

# üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
plt.figure(figsize=(12, 6))
bar_width = 0.25
x = np.arange(len(age_group_summary['age_group']))

plt.bar(x - bar_width, age_group_summary['avg_income'], width=bar_width, label='–°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥')
plt.bar(x, age_group_summary['avg_expenses'], width=bar_width, label='–°—Ä–µ–¥–Ω–∏–µ —Ä–∞—Å—Ö–æ–¥—ã')
plt.bar(x + bar_width, age_group_summary['avg_balance'], width=bar_width, label='–°—Ä–µ–¥–Ω–∏–π –æ—Å—Ç–∞—Ç–æ–∫')

plt.xticks(x, age_group_summary['age_group'])
plt.title("üìä –§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–º –≥—Ä—É–ø–ø–∞–º")
plt.xlabel("–í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞")
plt.ylabel("–°—É–º–º–∞ (‚Ç∏)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
X = encoded_df[['age', 'income', 'expenses', 'risk_profile_enc', 'investment_goal_enc']]
y = encoded_df['recommendation']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = RandomForestClassifier(random_state=42, class_weight='balanced')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# üìà –ú–µ—Ç—Ä–∏–∫–∏
print("\\nüìà –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
print(classification_report(y_test, y_pred))

# ü™¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —É–∑–ª—ã
age_groups = list(encoded_df['age_group'].astype(str).unique())
risk_profiles = ["–ù–∏–∑–∫–∏–π", "–°—Ä–µ–¥–Ω–∏–π", "–í—ã—Å–æ–∫–∏–π"]
recommendations = ["–û–±–ª–∏–≥–∞—Ü–∏–∏", "–ê–∫—Ü–∏–∏", "–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞"]

# –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ
G = nx.DiGraph()

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
from collections import Counter

sample_df = encoded_df.sample(5000, random_state=42)
group_to_risk = Counter()
risk_to_recommendation = Counter()

for _, row in sample_df.iterrows():
    group = str(row['age_group'])
    risk = row['risk_profile']
    rec = row['recommendation']
    group_to_risk[(group, risk)] += 1
    risk_to_recommendation[(risk, rec)] += 1

# –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏
edge_colors = []
edge_widths = []

for (group, risk), weight in group_to_risk.items():
    G.add_edge(group, risk, weight=weight)
    color = {"–ù–∏–∑–∫–∏–π": "green", "–°—Ä–µ–¥–Ω–∏–π": "orange", "–í—ã—Å–æ–∫–∏–π": "red"}[risk]
    edge_colors.append(color)
    edge_widths.append(weight / 150)

for (risk, rec), weight in risk_to_recommendation.items():
    G.add_edge(risk, rec, weight=weight)
    color = {"–ù–∏–∑–∫–∏–π": "green", "–°—Ä–µ–¥–Ω–∏–π": "orange", "–í—ã—Å–æ–∫–∏–π": "red"}[risk]
    edge_colors.append(color)
    edge_widths.append(weight / 150)

# –†—É—á–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∑–ª–æ–≤
pos = {}
y_age = 2
y_risk = 1
y_rec = 0

for i, group in enumerate(sorted(age_groups)):
    pos[group] = (i, y_age)

for i, risk in enumerate(risk_profiles):
    pos[risk] = (i + 1, y_risk)

for i, rec in enumerate(recommendations):
    pos[rec] = (i + 1, y_rec)

# –†–∏—Å—É–µ–º
plt.figure(figsize=(14, 8))
nx.draw(G, pos, with_labels=True, edge_color=edge_colors, width=edge_widths,
        node_size=2500, node_color="lightblue", font_size=10)

plt.title("üìä –ß—ë—Ç–∫–∞—è —Å—Ö–µ–º–∞: –í–æ–∑—Ä–∞—Å—Ç ‚Üí –†–∏—Å–∫ ‚Üí –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", fontsize=14)
plt.axis("off")
plt.text(2, 2.5,  # üìç x=7, y=2.5 ‚Äî –º–æ–∂–Ω–æ –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å
    "üé® –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Ü–≤–µ—Ç–∞ —Å—Ç—Ä–µ–ª–æ–∫:\\n"
    "üü• –ö—Ä–∞—Å–Ω—ã–π   ‚Üí –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ (–ö–ª–∏–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã —Ä–∏—Å–∫–æ–≤–∞—Ç—å) ‚Üí üí°–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞\\n"
    "üüß –û—Ä–∞–Ω–∂–µ–≤—ã–π ‚Üí –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ (–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ä–∏—Å–∫—É) ‚Üí üíº –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ê–∫—Ü–∏–∏\\n"
    "üü© –ó–µ–ª—ë–Ω—ã–π   ‚Üí –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ (–û—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ, –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)  ‚Üí üè¶ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –û–±–ª–∏–≥–∞—Ü–∏–∏",
    fontsize=10,
    bbox=dict(boxstyle="round", facecolor="white", edgecolor="gray")
)
plt.tight_layout()
plt.show()

```

### ‚úÖ **–ü—Ä–æ–µ–∫—Ç 2: AI-–°–ª—É–∂–±–∞ –ü–æ–¥–¥–µ—Ä–∂–∫–∏: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–∫–µ—Ç–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è** üí¨

üì¶ **–ß—Ç–æ –≤–Ω—É—Ç—Ä–∏:**

- **NLP** ‚Äî –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Ç–∏–∫–µ—Ç–æ–≤ (TF-IDF –∏–ª–∏ BERT-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏).
- **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–∏–∫–µ—Ç–æ–≤ (K-Means/DBSCAN)** ‚Äî –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã).
- **Decision Tree/Random Forest** ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–∫–µ—Ç–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –æ—Ç–¥–µ–ª–∞–º.

üìå **–ü–æ—á–µ–º—É —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:**

- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∏–∫–µ—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –≤ —Å–ª—É–∂–±–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∏.

```python
# üß† NLP-–ø—Ä–æ–µ–∫—Ç: "AI-–°–ª—É–∂–±–∞ –ü–æ–¥–¥–µ—Ä–∂–∫–∏"
# –¶–µ–ª—å: –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–∏–∫–µ—Ç—ã (–æ–±—Ä–∞—â–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤) –ø–æ –æ—Ç–¥–µ–ª–∞–º:
# ‚Üí –¢–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞, –§–∏–Ω–∞–Ω—Å—ã, –£–¥–∞–ª–µ–Ω–∏–µ –∞–∫–∫–∞—É–Ω—Ç–∞, –ñ–∞–ª–æ–±—ã, –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã

# üîß –ò–º–ø–æ—Ä—Ç—ã

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# ===============================
# üß† –®–∞–≥ 1: –î–∞—Ç–∞—Å–µ—Ç
# ===============================

categories = {
    "–¢–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞": [
        "–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Ö–æ–¥ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ",
        "–°–±–æ–π –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ",
        "–í—ã–¥–∞–µ—Ç –æ—à–∏–±–∫—É –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ"
    ],
    "–§–∏–Ω–∞–Ω—Å—ã": [
        "–û–ø–ª–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ –¥–≤–∞–∂–¥—ã",
        "–•–æ—á—É –≤–µ—Ä–Ω—É—Ç—å –¥–µ–Ω—å–≥–∏",
        "–°–ø–∏—Å–∞–ª–∏ –¥–µ–Ω—å–≥–∏ –±–µ–∑ –º–æ–µ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è"
    ],
    "–£–¥–∞–ª–µ–Ω–∏–µ": [
        "–ü—Ä–æ—à—É —É–¥–∞–ª–∏—Ç—å –º–æ–π –∞–∫–∫–∞—É–Ω—Ç",
        "–£–¥–∞–ª–∏—Ç–µ –º–æ–∏ –¥–∞–Ω–Ω—ã–µ",
        "–•–æ—á—É –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤—ã–π—Ç–∏ –∏–∑ —Å–∏—Å—Ç–µ–º—ã"
    ],
    "–ñ–∞–ª–æ–±—ã": [
        "–•–æ—á—É –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å—Å—è –Ω–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞",
        "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ —Å –∫–ª–∏–µ–Ω—Ç–æ–º",
        "–ù–µ –ø–æ–ª—É—á–∏–ª –æ–±–µ—â–∞–Ω–Ω—ã–π –±–æ–Ω—É—Å"
    ],
    "–û–±—â–∏–µ": [
        "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–æ–∫–æ–¥?",
        "–ì–¥–µ –Ω–∞–π—Ç–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é?",
        "–ï—Å—Ç—å –ª–∏ –º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è?"
    ]
}

data = {
    "ticket": [],
    "label": []
}

for label, phrases in categories.items():
    for phrase in phrases:
        data["ticket"].append(phrase)
        data["label"].append(label)

df = pd.DataFrame(data)

df = pd.DataFrame(data)

# ===============================
# üî† –®–∞–≥ 2: TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
# ===============================

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['ticket'])
y = df['label']

# ===============================
# üîÅ –®–∞–≥ 3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (KMeans)
# ===============================

kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(X)
df['cluster'] = clusters
print("\\nüìä –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–±–µ–∑ –º–µ—Ç–æ–∫):")
print(df[['ticket', 'cluster']])

from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

# PCA ‚Äî –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –∏–Ω–∞—á–µ DBSCAN –Ω–µ —Å–ø—Ä–∞–≤–∏—Ç—Å—è —Å TF-IDF
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X.toarray())

# DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
dbscan = DBSCAN(eps=0.5, min_samples=2)
clusters_db = dbscan.fit_predict(X_reduced)

df['dbscan_cluster'] = clusters_db
print(df[['ticket', 'dbscan_cluster']])

# ===============================
# üå≥ –®–∞–≥ 4: Random Forest
# ===============================

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
clf_rf = RandomForestClassifier()
clf_rf.fit(X_train, y_train)

y_pred_rf = clf_rf.predict(X_test)
print("\\nüß™ Random Forest:")
print(classification_report(y_test, y_pred_rf))

# ===============================
# üå≤ –®–∞–≥ 5: Decision Tree
# ===============================

clf_tree = DecisionTreeClassifier(random_state=42)
clf_tree.fit(X_train, y_train)

y_pred_tree = clf_tree.predict(X_test)
print("\\nüß™ Decision Tree:")
print(classification_report(y_test, y_pred_tree))

# ===============================
# üìâ –®–∞–≥ 6: Confusion Matrix
# ===============================

ConfusionMatrixDisplay.from_estimator(clf_rf, X_test, y_test, cmap='Blues')
plt.title("Confusion Matrix (Random Forest)")
plt.show()

# ===============================
# ü§ñ –®–∞–≥ 7: BERT-–≤–∞—Ä–∏–∞–Ω—Ç (–µ—Å–ª–∏ —Ö–æ—á–µ—à—å PRO-–≤–µ—Ä—Å–∏—é)
# ===============================

from sentence_transformers import SentenceTransformer

model_bert = SentenceTransformer('all-MiniLM-L6-v2')
X_bert = model_bert.encode(df['ticket'].tolist())

X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_bert, y,test_size=0.3, random_state=42)
clf_bert = RandomForestClassifier()
clf_bert.fit(X_train_b, y_train_b)

y_pred_bert = clf_bert.predict(X_test_b)
print("\\nü§ñ BERT + Random Forest:")
print(classification_report(y_test_b, y_pred_bert))
```

### ‚úÖ **–ü—Ä–æ–µ–∫—Ç 3: AI-–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π** üè•

üì¶ **–ß—Ç–æ –≤–Ω—É—Ç—Ä–∏:**

- **Convolutional Neural Networks (CNN)** ‚Äî –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–µ–Ω—Ç–≥–µ–Ω –∏–ª–∏ –ú–†–¢).
- **Data Augmentation** ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—ä—ë–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
- **Transfer Learning** ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, VGG16, ResNet) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

üìå **–ü–æ—á–µ–º—É —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:**

- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –º–µ–¥–∏—Ü–∏–Ω–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —Å—Ç–∞–¥–∏—è—Ö, —á—Ç–æ –º–æ–∂–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∂–∏–∑–Ω–∏ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –∏ —É—Å–∫–æ—Ä–∏—Ç—å –ª–µ—á–µ–Ω–∏–µ.

```python

# üî¨ –ü—Ä–æ–µ–∫—Ç 3: AI-–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
# üìç–¶–µ–ª—å: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
# üì¶ –ß—Ç–æ –±—É–¥–µ—Ç –≤–Ω—É—Ç—Ä–∏:

# üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Ä–µ–Ω—Ç–≥–µ–Ω / –ú–†–¢)
# üß† –û–±—É—á–µ–Ω–∏–µ CNN (Convolutional Neural Network)
# ‚öôÔ∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Transfer Learning (VGG16 / ResNet)
# üîÑ Data Augmentation (—É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏)
# üìà –ú–µ—Ç—Ä–∏–∫–∏: —Ç–æ—á–Ω–æ—Å—Ç—å, –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
# üß≠ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import glob
import os

# –ü—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
base_dir = r"C:\\Users\\Madina\\OneDrive\\–î–æ–∫—É–º–µ–Ω—Ç—ã\\Python Scripts\\train"
normal_dir = os.path.join(base_dir, "NORMAL")
pneumonia_dir = os.path.join(base_dir, "PNEUMONIA")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
normal_images = glob.glob(os.path.join(normal_dir, "*.jpeg"))[:3]
pneumonia_images = glob.glob(os.path.join(pneumonia_dir, "*.jpeg"))[:3]

# –°–æ–∑–¥–∞—ë–º –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
fig, axs = plt.subplots(2, 3, figsize=(12, 6))

for i, img_path in enumerate(normal_images):
    img = mpimg.imread(img_path)
    axs[0, i].imshow(img, cmap='gray')
    axs[0, i].set_title("NORMAL")
    axs[0, i].axis('off')

for i, img_path in enumerate(pneumonia_images):
    img = mpimg.imread(img_path)
    axs[1, i].imshow(img, cmap='Blues_r')
    axs[1, i].set_title("PNEUMONIA")
    axs[1, i].axis('on')

plt.suptitle("ü´Å Chest X-Ray: NORMAL vs PNEUMONIA", fontsize=16)
plt.tight_layout()
plt.show()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# üìÅ –ü—É—Ç—å –∫ —Ç–≤–æ–µ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –ø–∞–ø–∫–µ
train_dir = r"C:\\Users\\Madina\\OneDrive\\–î–æ–∫—É–º–µ–Ω—Ç—ã\\Python Scripts\\train"

# ‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π (–º–æ–∂–Ω–æ –±–µ–∑ –Ω–µ—ë –¥–ª—è –Ω–∞—á–∞–ª–∞)
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# üì§ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),   # –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±—É–¥—É—Ç 150x150 –ø–∏–∫—Å–µ–ª–µ–π
    batch_size=32, 
    class_mode='binary',  # –£ –Ω–∞—Å –¥–≤–∞ –∫–ª–∞—Å—Å–∞: 0 (NORMAL) –∏ 1 (PNEUMONIA)
    subset='training',
    color_mode='grayscale'  
)

# üì§ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=32, 
    class_mode='binary',  # –£ –Ω–∞—Å –¥–≤–∞ –∫–ª–∞—Å—Å–∞: 0 (NORMAL) –∏ 1 (PNEUMONIA)
    subset='validation',
    color_mode='grayscale'  
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# üéØ –ü—Ä–æ—Å—Ç–∞—è CNN-–º–æ–¥–µ–ª—å
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)), # 1 ‚Äî —Ç–∞–∫ –∫–∞–∫ grayscale
    MaxPooling2D(2, 2),
    
    Conv2D(64, (3, 3), activation='relu'), 
    MaxPooling2D(2, 2),
    
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator
)

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Accuracy –∏ Loss –ø–æ —ç–ø–æ—Ö–∞–º:
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title("üìà Accuracy –ø–æ —ç–ø–æ—Ö–∞–º")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("üìâ –ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# ‚úÖ –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞: AI-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–Ω—Ç–≥–µ–Ω—É

# –ò–º–ø–æ—Ä—Ç—ã
import os
import glob
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array, load_img

# === üîß –§—É–Ω–∫—Ü–∏–∏ ===

from PIL import Image

def avg_brightness(image_path):
    try:
        img = Image.open(image_path).convert('L')  # 'L' = grayscale
        return np.array(img).mean()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {image_path}: {e}")
        return 0

def get_feature_vector(image_path):
    img = load_img(image_path, target_size=(224, 224))
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    features = model.predict(img_array)
    return features.flatten()

# === üß† –ú–æ–¥–µ–ª—å VGG16 ===
model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# === üìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π ===
for category in ['NORMAL', 'PNEUMONIA']:
    folder_path = os.path.join(r"C:\\Users\\Madina\\OneDrive\\–î–æ–∫—É–º–µ–Ω—Ç—ã\\Python Scripts\\train", category)
    if not os.path.exists(folder_path):
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        continue
    paths = glob.glob(os.path.join(folder_path, "*.jpeg"))
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(paths)} —Ñ–∞–π–ª–æ–≤ –≤ {category}")

# === üìä –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ===
data = []

for category in ['NORMAL', 'PNEUMONIA']:
    folder_path = os.path.join(r"C:\\Users\\Madina\\OneDrive\\–î–æ–∫—É–º–µ–Ω—Ç—ã\\Python Scripts\\train", category)
    paths = glob.glob(os.path.join(folder_path, "*.jpeg"))
    sample_size = min(100, len(paths))
    sample_paths = random.sample(paths, sample_size)

    for path in sample_paths:
        brightness = avg_brightness(path)
        features = get_feature_vector(path)
        feature_sum = features.sum()
        data.append({
            "category": category,
            "path": path,
            "brightness": brightness,
            "feature_sum": feature_sum
        })

# === üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ===
df = pd.DataFrame(data)
anomalies = df[(df['brightness'] < 50) | (df['brightness'] > 200)]
print(anomalies[['category', 'path', 'brightness']])

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="brightness", y="feature_sum", hue="category", alpha=0.7)
plt.title("–ê–Ω–∞–ª–∏–∑ —Å–Ω–∏–º–∫–æ–≤: –Ø—Ä–∫–æ—Å—Ç—å vs –ü—Ä–∏–∑–Ω–∞–∫–∏ VGG16")
plt.xlabel("–°—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å")
plt.ylabel("–°—É–º–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (VGG16)")
plt.grid(True)
plt.tight_layout()
plt.show()

import plotly.express as px

df["filename"] = df["path"].apply(os.path.basename)

fig = px.scatter(
    df,
    x="brightness",
    y="feature_sum",
    color="category",
    hover_data=["filename", "category", "brightness", "feature_sum"],
    title="üß† –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–Ω–∏–º–∫–æ–≤: –Ø—Ä–∫–æ—Å—Ç—å vs –ü—Ä–∏–∑–Ω–∞–∫–∏ VGG16"
)

fig.show()

```